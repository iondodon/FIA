{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FIA_Lab_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE_jkLfwrs0j"
      },
      "source": [
        "Example 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6XF4jKIfn0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfe102b-9a10-41a5-f8f1-10dcc22abc8a"
      },
      "source": [
        "import keras \n",
        "import numpy as np\n",
        "from keras.datasets import boston_housing \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense \n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import RMSprop \n",
        "from keras.callbacks import EarlyStopping \n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import scale\n",
        "import pandas as pd\n",
        "\n",
        "pd_data = pd.read_csv(\n",
        "    'apartmentComplexData.txt', \n",
        "    delimiter=',',\n",
        "    names=['1', '2', '3', '4', '5', '6', '7', '8', '9']    \n",
        ")\n",
        "data = pd_data.to_numpy()\n",
        "\n",
        "train_data = data[: int(len(data) * 0.75)]\n",
        "train_variables = train_data[:, :8]\n",
        "train_values = train_data[:, 8]\n",
        "\n",
        "test_data = data[int(len(data) * 0.75) :]\n",
        "test_variables = test_data[:, :8]\n",
        "test_values = test_data[:, 8]\n",
        "\n",
        "x_train_scaled = preprocessing.scale(train_variables) \n",
        "scaler = preprocessing.StandardScaler().fit(train_variables) \n",
        "x_test_scaled = scaler.transform(test_variables)\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Dense(\n",
        "      1,\n",
        "      kernel_initializer = 'normal', \n",
        "      activation = 'relu',\n",
        "      input_shape = (8,)\n",
        "))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "sgd = keras.optimizers.SGD(learning_rate = 0.01)\n",
        "model.compile(loss='mae', optimizer=sgd, metrics=['mse'])\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "   x_train_scaled, train_values,    \n",
        "   batch_size = 128, \n",
        "   epochs = 100,\n",
        "   verbose = 1,\n",
        "   validation_split = 0.2,\n",
        "   callbacks = [EarlyStopping(monitor = 'val_loss', patience = 20)]\n",
        ")\n",
        "\n",
        "\n",
        "score = model.evaluate(x_train_scaled, train_values, verbose = 1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# prediction = model.predict(variables[0]) \n",
        "# print(prediction.flatten()) \n",
        "# print(y_test)\n",
        "\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "97/97 [==============================] - 1s 3ms/step - loss: 209828.8659 - mse: 57274447704.8163 - val_loss: 161973.5156 - val_mse: 33975801856.0000\n",
            "Epoch 2/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206825.2808 - mse: 55668131045.8775 - val_loss: 161972.6719 - val_mse: 33975545856.0000\n",
            "Epoch 3/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207831.4283 - mse: 56535376791.5102 - val_loss: 161971.8594 - val_mse: 33975281664.0000\n",
            "Epoch 4/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209709.0073 - mse: 57073253062.5306 - val_loss: 161971.0469 - val_mse: 33975025664.0000\n",
            "Epoch 5/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208152.7181 - mse: 56614800446.6939 - val_loss: 161970.2344 - val_mse: 33974761472.0000\n",
            "Epoch 6/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209048.1674 - mse: 56921015442.2857 - val_loss: 161969.4219 - val_mse: 33974509568.0000\n",
            "Epoch 7/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207740.4810 - mse: 56129278411.7551 - val_loss: 161968.6094 - val_mse: 33974253568.0000\n",
            "Epoch 8/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208147.6401 - mse: 56480962309.2245 - val_loss: 161967.7969 - val_mse: 33973995520.0000\n",
            "Epoch 9/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207872.7468 - mse: 56179136637.3878 - val_loss: 161967.0156 - val_mse: 33973737472.0000\n",
            "Epoch 10/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208998.3233 - mse: 56831601434.1225 - val_loss: 161966.2031 - val_mse: 33973483520.0000\n",
            "Epoch 11/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208031.5827 - mse: 56372887343.0204 - val_loss: 161965.3750 - val_mse: 33973231616.0000\n",
            "Epoch 12/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207704.5855 - mse: 56131482603.1020 - val_loss: 161964.5938 - val_mse: 33972975616.0000\n",
            "Epoch 13/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206903.5818 - mse: 55854133749.5510 - val_loss: 161963.7969 - val_mse: 33972719616.0000\n",
            "Epoch 14/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206608.2411 - mse: 55601036726.8571 - val_loss: 161962.9844 - val_mse: 33972467712.0000\n",
            "Epoch 15/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208621.2738 - mse: 56707101842.2857 - val_loss: 161962.1562 - val_mse: 33972209664.0000\n",
            "Epoch 16/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208881.9844 - mse: 56942241917.3878 - val_loss: 161961.3594 - val_mse: 33971963904.0000\n",
            "Epoch 17/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206954.3071 - mse: 55948431443.5918 - val_loss: 161960.5625 - val_mse: 33971699712.0000\n",
            "Epoch 18/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208084.9882 - mse: 56612696064.0000 - val_loss: 161959.7656 - val_mse: 33971447808.0000\n",
            "Epoch 19/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208917.7693 - mse: 56747824901.2245 - val_loss: 161958.9531 - val_mse: 33971193856.0000\n",
            "Epoch 20/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208181.4144 - mse: 56560473443.2653 - val_loss: 161958.1562 - val_mse: 33970941952.0000\n",
            "Epoch 21/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208271.7862 - mse: 56448446881.9592 - val_loss: 161957.3281 - val_mse: 33970690048.0000\n",
            "Epoch 22/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209847.2809 - mse: 57664605455.6735 - val_loss: 161956.5312 - val_mse: 33970429952.0000\n",
            "Epoch 23/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207461.8548 - mse: 56120875216.9796 - val_loss: 161955.7344 - val_mse: 33970182144.0000\n",
            "Epoch 24/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207270.0430 - mse: 56333319397.8775 - val_loss: 161954.9219 - val_mse: 33969930240.0000\n",
            "Epoch 25/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208546.2865 - mse: 56617569802.4490 - val_loss: 161954.1250 - val_mse: 33969676288.0000\n",
            "Epoch 26/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 210834.6424 - mse: 57933352124.0816 - val_loss: 161953.3281 - val_mse: 33969424384.0000\n",
            "Epoch 27/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207209.3788 - mse: 55929643133.3878 - val_loss: 161952.5312 - val_mse: 33969170432.0000\n",
            "Epoch 28/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209620.2564 - mse: 57328165365.5510 - val_loss: 161951.7031 - val_mse: 33968912384.0000\n",
            "Epoch 29/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207484.7262 - mse: 56236539068.0816 - val_loss: 161950.9219 - val_mse: 33968658432.0000\n",
            "Epoch 30/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207891.9149 - mse: 56314746420.2449 - val_loss: 161950.0938 - val_mse: 33968402432.0000\n",
            "Epoch 31/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207755.5150 - mse: 56208131887.0204 - val_loss: 161949.3125 - val_mse: 33968150528.0000\n",
            "Epoch 32/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 210095.3152 - mse: 57546185748.8980 - val_loss: 161948.5000 - val_mse: 33967896576.0000\n",
            "Epoch 33/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207430.3699 - mse: 55948250676.2449 - val_loss: 161947.7031 - val_mse: 33967644672.0000\n",
            "Epoch 34/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208151.0996 - mse: 56477627245.7143 - val_loss: 161946.8906 - val_mse: 33967388672.0000\n",
            "Epoch 35/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208224.9678 - mse: 56480796797.3878 - val_loss: 161946.0625 - val_mse: 33967136768.0000\n",
            "Epoch 36/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209977.9061 - mse: 57475340914.9388 - val_loss: 161945.2656 - val_mse: 33966880768.0000\n",
            "Epoch 37/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208858.0759 - mse: 56807425212.0816 - val_loss: 161944.4531 - val_mse: 33966630912.0000\n",
            "Epoch 38/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209859.2489 - mse: 57211622504.4898 - val_loss: 161943.6562 - val_mse: 33966376960.0000\n",
            "Epoch 39/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207180.4290 - mse: 56168507935.3469 - val_loss: 161942.8750 - val_mse: 33966116864.0000\n",
            "Epoch 40/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206847.1515 - mse: 55574141220.5714 - val_loss: 161942.0469 - val_mse: 33965864960.0000\n",
            "Epoch 41/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209381.0863 - mse: 57011973349.8775 - val_loss: 161941.2500 - val_mse: 33965611008.0000\n",
            "Epoch 42/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207376.9399 - mse: 56298603917.0612 - val_loss: 161940.4375 - val_mse: 33965355008.0000\n",
            "Epoch 43/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207508.7467 - mse: 56197673670.5306 - val_loss: 161939.6562 - val_mse: 33965107200.0000\n",
            "Epoch 44/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206009.7127 - mse: 55340155674.1225 - val_loss: 161938.8438 - val_mse: 33964851200.0000\n",
            "Epoch 45/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207060.0445 - mse: 55801108229.2245 - val_loss: 161938.0312 - val_mse: 33964599296.0000\n",
            "Epoch 46/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209412.2294 - mse: 57104459358.0408 - val_loss: 161937.2344 - val_mse: 33964343296.0000\n",
            "Epoch 47/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207627.0584 - mse: 56238060314.1225 - val_loss: 161936.4375 - val_mse: 33964091392.0000\n",
            "Epoch 48/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206711.8645 - mse: 55824102713.4694 - val_loss: 161935.6250 - val_mse: 33963839488.0000\n",
            "Epoch 49/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207093.0499 - mse: 55966941351.1837 - val_loss: 161934.8281 - val_mse: 33963587584.0000\n",
            "Epoch 50/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209272.5678 - mse: 57065309246.6939 - val_loss: 161934.0312 - val_mse: 33963333632.0000\n",
            "Epoch 51/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208022.9884 - mse: 56311931088.9796 - val_loss: 161933.2031 - val_mse: 33963081728.0000\n",
            "Epoch 52/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207830.3874 - mse: 56271734491.4286 - val_loss: 161932.4062 - val_mse: 33962817536.0000\n",
            "Epoch 53/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 210630.3616 - mse: 57757625866.4490 - val_loss: 161931.5938 - val_mse: 33962569728.0000\n",
            "Epoch 54/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208994.8444 - mse: 56893866109.3878 - val_loss: 161930.7969 - val_mse: 33962317824.0000\n",
            "Epoch 55/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209605.4721 - mse: 57228794524.7347 - val_loss: 161929.9844 - val_mse: 33962061824.0000\n",
            "Epoch 56/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 205903.3882 - mse: 55107712397.0612 - val_loss: 161929.1719 - val_mse: 33961809920.0000\n",
            "Epoch 57/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208302.6406 - mse: 56661017286.5306 - val_loss: 161928.3750 - val_mse: 33961564160.0000\n",
            "Epoch 58/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208362.1143 - mse: 56600961191.1837 - val_loss: 161927.5938 - val_mse: 33961304064.0000\n",
            "Epoch 59/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209185.3967 - mse: 56697879071.3469 - val_loss: 161926.7812 - val_mse: 33961050112.0000\n",
            "Epoch 60/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207972.5810 - mse: 56125467543.5102 - val_loss: 161925.9688 - val_mse: 33960798208.0000\n",
            "Epoch 61/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207209.8021 - mse: 55958482275.2653 - val_loss: 161925.1562 - val_mse: 33960548352.0000\n",
            "Epoch 62/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207729.1055 - mse: 56344212124.7347 - val_loss: 161924.3750 - val_mse: 33960292352.0000\n",
            "Epoch 63/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209745.9388 - mse: 57024547401.1429 - val_loss: 161923.5625 - val_mse: 33960040448.0000\n",
            "Epoch 64/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207580.5710 - mse: 56366066541.7143 - val_loss: 161922.7656 - val_mse: 33959784448.0000\n",
            "Epoch 65/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208138.0249 - mse: 56473360509.3878 - val_loss: 161921.9844 - val_mse: 33959534592.0000\n",
            "Epoch 66/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209303.7023 - mse: 57048277284.5714 - val_loss: 161921.1719 - val_mse: 33959276544.0000\n",
            "Epoch 67/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209581.0515 - mse: 57186929141.5510 - val_loss: 161920.3281 - val_mse: 33959024640.0000\n",
            "Epoch 68/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207756.8358 - mse: 56121584075.7551 - val_loss: 161919.5469 - val_mse: 33958772736.0000\n",
            "Epoch 69/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 210193.6451 - mse: 57452875023.6735 - val_loss: 161918.7500 - val_mse: 33958520832.0000\n",
            "Epoch 70/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209220.4053 - mse: 57033389871.0204 - val_loss: 161917.9375 - val_mse: 33958264832.0000\n",
            "Epoch 71/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208252.7076 - mse: 56720470475.7551 - val_loss: 161917.1406 - val_mse: 33958010880.0000\n",
            "Epoch 72/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209411.7761 - mse: 57211852507.4286 - val_loss: 161916.3281 - val_mse: 33957758976.0000\n",
            "Epoch 73/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207020.4746 - mse: 55811927813.2245 - val_loss: 161915.5312 - val_mse: 33957505024.0000\n",
            "Epoch 74/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207767.7594 - mse: 56104293605.8775 - val_loss: 161914.7188 - val_mse: 33957251072.0000\n",
            "Epoch 75/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207857.5116 - mse: 56126751430.5306 - val_loss: 161913.9062 - val_mse: 33956999168.0000\n",
            "Epoch 76/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208639.0403 - mse: 56511461919.3469 - val_loss: 161913.1094 - val_mse: 33956747264.0000\n",
            "Epoch 77/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209164.0757 - mse: 57023023814.5306 - val_loss: 161912.3281 - val_mse: 33956497408.0000\n",
            "Epoch 78/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208120.8197 - mse: 56347443367.1837 - val_loss: 161911.4844 - val_mse: 33956243456.0000\n",
            "Epoch 79/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208474.2887 - mse: 56683334593.3061 - val_loss: 161910.7031 - val_mse: 33955987456.0000\n",
            "Epoch 80/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209105.0544 - mse: 56948507000.1633 - val_loss: 161909.9062 - val_mse: 33955737600.0000\n",
            "Epoch 81/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208766.4877 - mse: 56910008069.2245 - val_loss: 161909.0781 - val_mse: 33955485696.0000\n",
            "Epoch 82/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207307.3973 - mse: 55776110215.8367 - val_loss: 161908.2969 - val_mse: 33955231744.0000\n",
            "Epoch 83/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207174.5410 - mse: 55900362919.1837 - val_loss: 161907.5000 - val_mse: 33954977792.0000\n",
            "Epoch 84/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206980.0764 - mse: 55769032871.1837 - val_loss: 161906.7031 - val_mse: 33954725888.0000\n",
            "Epoch 85/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206048.8693 - mse: 55414237727.3469 - val_loss: 161905.8750 - val_mse: 33954469888.0000\n",
            "Epoch 86/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208631.5016 - mse: 56900944331.7551 - val_loss: 161905.0781 - val_mse: 33954222080.0000\n",
            "Epoch 87/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209267.1290 - mse: 56988771056.3265 - val_loss: 161904.3125 - val_mse: 33953970176.0000\n",
            "Epoch 88/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209694.4987 - mse: 57394472207.6735 - val_loss: 161903.4688 - val_mse: 33953714176.0000\n",
            "Epoch 89/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206914.1623 - mse: 55873803911.8367 - val_loss: 161902.6875 - val_mse: 33953468416.0000\n",
            "Epoch 90/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208541.2833 - mse: 56833368858.1225 - val_loss: 161901.9062 - val_mse: 33953208320.0000\n",
            "Epoch 91/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208785.5171 - mse: 56893768892.0816 - val_loss: 161901.0781 - val_mse: 33952950272.0000\n",
            "Epoch 92/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209183.4447 - mse: 57016157664.6531 - val_loss: 161900.2500 - val_mse: 33952704512.0000\n",
            "Epoch 93/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208391.0796 - mse: 56585031847.1837 - val_loss: 161899.4844 - val_mse: 33952448512.0000\n",
            "Epoch 94/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208033.5193 - mse: 56504500850.9388 - val_loss: 161898.6719 - val_mse: 33952196608.0000\n",
            "Epoch 95/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209100.5113 - mse: 56790878584.1633 - val_loss: 161897.8594 - val_mse: 33951944704.0000\n",
            "Epoch 96/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209495.3917 - mse: 57131103587.2653 - val_loss: 161897.0469 - val_mse: 33951692800.0000\n",
            "Epoch 97/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 206014.6068 - mse: 55352609771.1020 - val_loss: 161896.2656 - val_mse: 33951440896.0000\n",
            "Epoch 98/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 209449.4654 - mse: 57231868196.5714 - val_loss: 161895.4688 - val_mse: 33951188992.0000\n",
            "Epoch 99/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 208187.3697 - mse: 56463905603.9184 - val_loss: 161894.6562 - val_mse: 33950935040.0000\n",
            "Epoch 100/100\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 207808.5861 - mse: 56303537507.2653 - val_loss: 161893.8750 - val_mse: 33950683136.0000\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 198909.0000 - mse: 51951165440.0000\n",
            "Test loss: 198909.0\n",
            "Test accuracy: 51951165440.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0wNCYYolmHl"
      },
      "source": [
        "Example 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9RdtK5ElnbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60642325-b3cd-4ddf-e3a3-0b34fbff53ac"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd_data = pd.read_csv(\n",
        "    'apartmentComplexData.txt', \n",
        "    delimiter=',',\n",
        "    names=['1', '2', '3', '4', '5', '6', '7', '8', '9']    \n",
        ")\n",
        "\n",
        "msk = np.random.rand(len(pd_data)) < 0.8\n",
        "train_data = pd_data[msk]\n",
        "test_data = pd_data[~msk]\n",
        "\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(train_data[['1','2','3','4','5','6','7','8']], train_data[['9']])\n",
        "\n",
        "test_predictions = reg.predict(test_data[['1','2','3','4','5','6','7','8']])\n",
        "\n",
        "y_true = test_data[['9']].to_numpy()[:,0]\n",
        "y_pred = test_predictions[:,0]\n",
        "\n",
        "variance = explained_variance_score(y_true, y_pred)\n",
        "print(\"explained_variance_score = \", variance)\n",
        "\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(\"mean_squared_error = \", mse)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(\"mean_absolute_error = \", mae)\n",
        "\n",
        "score = reg.score(test_data[['1','2','3','4','5','6','7','8']], y_true)\n",
        "print(\"score = \", score)\n",
        "\n",
        "\n",
        "# plt.scatter(test_data[['7']], test_data[['9']],  color='gainsboro')\n",
        "# plt.plot(test_data[['7']], test_predictions, color='royalblue', linewidth = 1, linestyle= '-')\n",
        "# plt.show()\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explained_variance_score =  0.6148642629083725\n",
            "mean_squared_error =  4946683796.4902525\n",
            "mean_absolute_error =  50826.139830448\n",
            "score =  0.6148526639096303\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}